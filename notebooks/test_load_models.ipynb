{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_load_models.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"rZ0OiBu1yIPY","colab_type":"code","colab":{}},"source":["os.chdir('drive/My Drive/ift6759/ift6759-t6-p2')\n","os.getcwd()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCQwJ6G2yB6C","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import click\n","import json\n","\n","from sklearn.model_selection import train_test_split\n","from utils.data import preprocess_v2id\n","from seq_2_seq_models.seq_2_seq import seq_2_seq_GRU, Loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4lM9SUYyykz","colab_type":"code","colab":{}},"source":["Loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_M6Zb-MzIHm","colab_type":"code","colab":{}},"source":["path = os.path.join(os.getcwd(), 'seq_2_seq_models', 'unformated_en_2_unformated_fr_w2w')\n","path = os.path.join(path, 'GRU_2.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3OJGlNuz23u","colab_type":"code","colab":{}},"source":["def train(encoder_lang_model_task, decoder_lang_model_task, config_path,\n","          batch_size, epochs, checkpoint_epoch, train_split_ratio,\n","          steps_per_epoch, model_name):\n","\n","    with open(config_path, \"r\") as fd:\n","        config = json.load(fd)\n","\n","    BATCH_SIZE = batch_size\n","    encoder_checkpoint_file = model_name + \"_{}.h5\".format(checkpoint_epoch)\n","    decoder_checkpoint_file = model_name + \"_{}.h5\".format(checkpoint_epoch)\n","\n","    # Directory where the checkpoints will be saved\n","    checkpoint_dir = os.path.join(\n","        os.getcwd(),\n","        'seq_2_seq_models',\n","        encoder_lang_model_task[:-4] + \"_2_\" +\n","        decoder_lang_model_task[:-4] + \"_\" +\n","        encoder_lang_model_task[-1] + \"2\" +\n","        decoder_lang_model_task[-1]\n","    )\n","    checkpoint_prefix = os.path.join(\n","        os.getcwd(),\n","        checkpoint_dir,\n","        model_name + \"_{epoch}.h5\"\n","    )\n","    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_prefix,\n","        save_weights_only=False)\n","\n","    '''\n","    Encoder-Decoder Config Loading:\n","        max_seq, vocab_size, remove_punctuation, tokenize_type,\n","        data_file, embedding_dim, units, lang_model_checkpointer\n","    '''\n","    encoder_config = config[encoder_lang_model_task]\n","    decoder_config = config[decoder_lang_model_task]\n","\n","    encoder_config['tokenize_type'] = list(encoder_lang_model_task)[-1]\n","    encoder_config['data_file'] = os.path.join(\n","        \"data\",\n","        \"aligned_\" + encoder_lang_model_task.split(\"_\")[0] + \"_\" +\n","        encoder_lang_model_task.split(\"_\")[1]\n","    )\n","    encoder_config['lang_model_checkpointer'] = os.path.join(\n","        \"language_models\",\n","        encoder_lang_model_task,\n","        encoder_checkpoint_file\n","    )\n","\n","    decoder_config['tokenize_type'] = list(decoder_lang_model_task)[-1]\n","    decoder_config['data_file'] = os.path.join(\n","        \"data\",\n","        \"aligned_\" + decoder_lang_model_task.split(\"_\")[0] +\n","        \"_\" + decoder_lang_model_task.split(\"_\")[1]\n","    )\n","    decoder_config['lang_model_checkpointer'] = os.path.join(\n","        \"language_models\",\n","        decoder_lang_model_task,\n","        decoder_checkpoint_file\n","    )\n","\n","    # dataset\n","    encoder_v2id, encoder_dataset = preprocess_v2id(\n","        data=os.path.join(os.getcwd(), encoder_config['data_file']),\n","        v2id=os.path.join(\n","            os.getcwd(),\n","            \"language_models\",\n","            encoder_lang_model_task,\n","            \"v2id.json\"\n","        ),\n","        tokenize_type=encoder_config['tokenize_type'],\n","        max_seq=encoder_config['max_seq'],\n","        remove_punctuation=encoder_config['remove_punctuation']\n","    )\n","\n","    decoder_v2id, decoder_dataset = preprocess_v2id(\n","        data=os.path.join(os.getcwd(), decoder_config['data_file']),\n","        v2id=os.path.join(\n","            os.getcwd(),\n","            \"language_models\",\n","            decoder_lang_model_task,\n","            \"v2id.json\"\n","        ),\n","        tokenize_type=decoder_config['tokenize_type'],\n","        max_seq=decoder_config['max_seq'],\n","        remove_punctuation=decoder_config['remove_punctuation']\n","    )\n","\n","    (\n","        input_tensor_train,\n","        input_tensor_valid,\n","        target_tensor_train,\n","        target_tensor_valid\n","    ) = train_test_split(\n","        encoder_dataset, decoder_dataset, test_size=train_split_ratio)\n","\n","    BUFFER_SIZE = len(input_tensor_train)\n","    if steps_per_epoch is None:\n","        steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","    vocab_inp_size = len(encoder_v2id)\n","    vocab_tar_size = len(decoder_v2id)\n","\n","    encoder_config['max_seq'] = encoder_dataset.shape[-1]\n","    decoder_config['max_seq'] = decoder_dataset.shape[-1]\n","\n","    encoder_config['vocab_size'] = vocab_inp_size\n","    decoder_config['vocab_size'] = vocab_tar_size\n","\n","    print(\"#### ENC-DEC DATA Preprocessed ####\")\n","    print(\"Enc:\", encoder_config)\n","    print(\"Dec:\", decoder_config)\n","\n","    dataset_train = tf.data.Dataset.from_tensor_slices(\n","        ((input_tensor_train, target_tensor_train), target_tensor_train)\n","    ).shuffle(BUFFER_SIZE)\n","    dataset_train = dataset_train.batch(\n","        BATCH_SIZE,\n","        drop_remainder=True).repeat()\n","\n","    dataset_valid = tf.data.Dataset.from_tensor_slices(\n","        ((input_tensor_valid, target_tensor_valid), target_tensor_valid)\n","    ).shuffle(BUFFER_SIZE)\n","    dataset_valid = dataset_valid.batch(\n","        BATCH_SIZE, drop_remainder=True\n","    ).repeat()\n","\n","    print(\"#### Datasets Loaded ####\")\n","    print(dataset_train, dataset_valid)\n","\n","    def get_model():\n","        seq_2_seq_model = seq_2_seq_GRU(\n","            vocab_inp_size=vocab_inp_size,\n","            encoder_embedding_dim=encoder_config['embedding_dim'],\n","            encoder_units=encoder_config['units'],\n","            vocab_tar_size=vocab_tar_size,\n","            decoder_embedding_dim=decoder_config['embedding_dim'],\n","            decoder_units=decoder_config['units'],\n","            decoder_v2id=decoder_v2id,\n","            targ_seq_len=decoder_config['max_seq'],\n","            BATCH_SIZE=BATCH_SIZE,\n","            encoder_lang_model=encoder_config['lang_model_checkpointer'],\n","            decoder_lang_model=decoder_config['lang_model_checkpointer']\n","        )\n","\n","        optimizer = tf.keras.optimizers.Adam()\n","\n","        seq_2_seq_model.compile(\n","            optimizer=optimizer, loss=Loss, run_eagerly=True\n","        )\n","\n","        return seq_2_seq_model\n","\n","    seq_2_seq_model = get_model()\n","\n","    print(\"#### Model Loaded ####\")\n","\n","    return seq_2_seq_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVUZRtOw0Xt0","colab_type":"code","outputId":"5d1fd93e-359a-42d9-b75f-c26d7d4b7996","executionInfo":{"status":"ok","timestamp":1586215974037,"user_tz":240,"elapsed":23146,"user":{"displayName":"Ankur Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_6W6aMEfbSSR-gH2aCHOTUvwO86Ern6Dx4xlY=s64","userId":"04283221963854590469"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["m = train(\n","          encoder_lang_model_task='unformated_en_w2w',\n","          decoder_lang_model_task='unformated_fr_w2w',\n","          config_path='config/language_models.json',\n","          batch_size=64,\n","          epochs=20,\n","          checkpoint_epoch=30,\n","          train_split_ratio=0.2,\n","          steps_per_epoch=None,\n","          model_name='GRU'\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 11000/11000 [00:00<00:00, 167365.61it/s]\n","100%|██████████| 11000/11000 [00:00<00:00, 141618.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["#### ENC-DEC DATA Preprocessed ####\n","Enc: {'max_seq': 50, 'vocab_size': 72116, 'remove_punctuation': True, 'embedding_dim': 128, 'units': 128, 'tokenize_type': 'w', 'data_file': 'data/aligned_unformated_en', 'lang_model_checkpointer': 'language_models/unformated_en_w2w/GRU_30.h5'}\n","Dec: {'max_seq': 50, 'vocab_size': 91257, 'remove_punctuation': True, 'embedding_dim': 128, 'units': 128, 'tokenize_type': 'w', 'data_file': 'data/aligned_unformated_fr', 'lang_model_checkpointer': 'language_models/unformated_fr_w2w/GRU_30.h5'}\n","#### Datasets Loaded ####\n","<RepeatDataset shapes: (((64, 50), (64, 50)), (64, 50)), types: ((tf.int32, tf.int32), tf.int32)> <RepeatDataset shapes: (((64, 50), (64, 50)), (64, 50)), types: ((tf.int32, tf.int32), tf.int32)>\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n","#### Model Loaded ####\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wVSzcLoeyR_E","colab_type":"code","outputId":"db640b65-ae0e-43fd-bc79-58afde75e0b6","executionInfo":{"status":"error","timestamp":1586216007514,"user_tz":240,"elapsed":3935,"user":{"displayName":"Ankur Agarwal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh_6W6aMEfbSSR-gH2aCHOTUvwO86Ern6Dx4xlY=s64","userId":"04283221963854590469"}},"colab":{"base_uri":"https://localhost:8080/","height":326}},"source":["m.load_model(path)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-9be27d86cc9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1260\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    700\u001b[0m                        \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                        \u001b[0;34m' weights, but the saved weights have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                        str(len(weight_values)) + ' elements.')\n\u001b[0m\u001b[1;32m    703\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Layer #1 (named \"decoder_gru\" in the current model) was found to correspond to layer decoder_gru in the save file. However the new layer decoder_gru expects 4 weights, but the saved weights have 14 elements."]}]},{"cell_type":"code","metadata":{"id":"PRicFG4KzcHH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}