{
  "lr": 0.001,
  "dr": 0.1,
  "batch_size": 4,
  "epochs": 10,
  "num_layers": 1,
  "atten_dim": 8,
  "num_heads": 2,
  "ff_dim": 64,
  "checkpoints_path": "saved_models/checkpoints",
  "steps_per_epoch": 2,
  "best_model_path": "saved_models",
  "model_name": "transformer"
}